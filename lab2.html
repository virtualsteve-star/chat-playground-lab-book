<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lab 2: Broken Bot - Steve's Chat Playground Lab Book</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Lab 2: Broken Bot</h1>
        <p class="subtitle">Understanding Security Vulnerabilities and Guardrails</p>
    </header>

    <nav>
        <ul>
            <li><a href="index.html">← Back to Index</a></li>
            <li><a href="lab1.html">← Previous Lab</a></li>
            <li><a href="lab3.html">Next Lab →</a></li>
            <li><a href="https://github.com/virtualsteve-star/chat-playground" target="_blank">GitHub Repo</a></li>
            <li><a href="https://virtualsteve-star.github.io/chat-playground/" target="_blank">Live Demo</a></li>
        </ul>
    </nav>

    <main>
        <section>
            <h2>Lab Overview</h2>
            <p>Now things get interesting! Meet Oscar, our intentionally "broken" bot who demonstrates what can happen when AI systems go wrong. To understand why this matters, let's look at a real-world disaster: Microsoft's Tay chatbot. In 2016, Microsoft released Tay on Twitter as an experiment in conversational AI. Within hours, internet users had "hacked" Tay by feeding it racist and inflammatory content, which the bot then repeated and amplified. The result? Bad headlines, public embarrassment, and Microsoft pulling Tay offline after just 16 hours.</p>
            <p>The tragedy is that this was entirely preventable. Simple content filters could have stopped Tay from repeating harmful language, saving Microsoft from a PR nightmare and protecting users from exposure to inappropriate content. In this lab, you'll see exactly how Microsoft could have saved itself - and how you can prevent similar disasters in your own AI systems.</p>
            <p>This lab is your first taste of AI security in action. You'll witness firsthand how simple guardrails can transform a potentially dangerous AI system into a safe, controlled one. Through hands-on experimentation, you'll learn why content filtering isn't just a nice-to-have feature - it's essential for any AI system that interacts with users. We'll show you how basic keyword filters work, why they're important, and how they can be the difference between a helpful AI assistant and a liability. By the end, you'll understand the fundamental principle that every AI system needs multiple layers of protection, and you'll have the tools to implement the first line of defense.</p>
            <div class="lab-meta">
                <span class="skill-level">Skill Level: 1</span>
                <span class="prereqs">Prerequisites: None</span>
            </div>
        </section>

        <section>
            <h2>Exercises</h2>

            <div class="exercise">
                <h4>Exercise 2.A: Meet Oscar</h4>
                <div class="exercise-meta">
                    <span>Skill Level: 1</span>
                    <span>Prerequisites: None</span>
                </div>
                <div class="directions">
                    <h5>Directions:</h5>
                    <p>Go to the <a href="https://virtualsteve-star.github.io/chat-playground/" target="_blank">live app</a>. Choose Oscar from the Bot picker. Have a conversation. Note that he isn't very charming - he's simulating a jailbroken bot like Tay from Chapter 1 of Steve's book.</p>
                </div>
            </div>

            <div class="exercise">
                <h4>Exercise 2.B: Analyze Oscar</h4>
                <div class="exercise-meta">
                    <span>Skill Level: 1</span>
                    <span>Prerequisites: None</span>
                </div>
                <div class="directions">
                    <h5>Directions:</h5>
                    <p>Inspect his ruleset to see where his charming behavior comes from.</p>
                    <p><a href="https://github.com/virtualsteve-star/chat-playground/blob/main/personalities/vuln_rude_rules.txt" target="_blank">View Oscar's Rules</a></p>
                </div>
            </div>

            <div class="exercise">
                <h4>Exercise 2.C: Your First Guardrails</h4>
                <div class="exercise-meta">
                    <span>Skill Level: 1</span>
                    <span>Prerequisites: None</span>
                </div>
                <div class="directions">
                    <h5>Directions:</h5>
                    <p>Open the guardrails panel using the toolbar button (find it with the tooltips if you're having trouble). Check the boxes for the Sex (Local) and Violence (Local) input and output filters. Now have a conversation. Trigger Oscar's destructive behaviors, feel free to get nasty with him too. Watch the guardrails in action.</p>
                </div>
            </div>

            <div class="exercise">
                <h4>Exercise 2.D: Inspect the Guardrails</h4>
                <div class="exercise-meta">
                    <span>Skill Level: 1 (2 for extra credit)</span>
                    <span>Prerequisites: None</span>
                </div>
                <div class="directions">
                    <h5>Directions:</h5>
                    <p>Go to the blocklists and see how they're constructed. Note, these are very simple for demo purposes.</p>
                    <p><a href="https://github.com/virtualsteve-star/chat-playground/blob/main/scripts/filters/violence_blocklist.txt" target="_blank">View Violence Blocklist</a></p>
                    <p><a href="https://github.com/virtualsteve-star/chat-playground/blob/main/scripts/filters/sex_blocklist.txt" target="_blank">View Sex Blocklist</a></p>
                </div>
                <div class="extra-credit">
                    <h5>Extra Credit:</h5>
                    <p>Inspect the JavaScript that implements the blocklists.</p>
                    <p><a href="https://github.com/virtualsteve-star/chat-playground/blob/main/scripts/filters/blocklist.js" target="_blank">View Blocklist Implementation</a></p>
                </div>
                <div class="extra-credit">
                    <h5>Extra, Extra Credit:</h5>
                    <p>Clone the repo and make your own copy on your machine. Expand the blocklists to include more terms or terms in other languages. Reload the app and test your expanded guardrails.</p>
                </div>
            </div>
        </section>

        <section>
            <h2>Key Learning Points</h2>
            <ul>
                <li>Understanding how vulnerable AI systems can behave</li>
                <li>Learning about basic content filtering mechanisms</li>
                <li>Experiencing guardrails in action</li>
                <li>Understanding the importance of input and output filtering</li>
                <li>Seeing how simple blocklists can provide basic protection</li>
            </ul>
        </section>

        <section>
            <h2>Next Steps</h2>
            <p>Once you've completed these exercises, you'll be ready to move on to <a href="lab3.html">Lab 3: Locking the Front Door and Back Door</a>, where you'll learn about prompt injection attacks and more advanced security measures.</p>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Steve's Chat Playground Lab Book. Based on the <a href="https://github.com/virtualsteve-star/chat-playground" target="_blank">Chat Playground Project</a>.</p>
    </footer>
</body>
</html> 